{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf556d6e-5edf-4e27-9d9f-751211a5193c",
   "metadata": {},
   "source": [
    "**Load Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cbedde7-b827-4928-baf7-d037e5a47f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c41c15fc-f8c1-4990-b8e6-530c6894e11b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting all the files now...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from zipfile import ZipFile\n",
    "\n",
    "with ZipFile(\"c4_imdb-8k.zip\") as zip:\n",
    "    print(\"Extracting all the files now...\")\n",
    "    zip.extractall()\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f390ee40-2ec3-47d4-9373-8681edbcd3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files\n",
    "\n",
    "# Path to datafolder\n",
    "PATH_DATA = \"c4_imdb-8k\"\n",
    "\n",
    "# load training data\n",
    "train_data = load_files(\n",
    "    os.path.join(PATH_DATA, \"train\"), encoding=\"ISO-8859-1\", categories=[\"pos\", \"neg\"]\n",
    ")\n",
    "\n",
    "# Load test data\n",
    "test_data = load_files(\n",
    "    os.path.join(PATH_DATA, \"test\"), encoding=\"ISO-8859-1\", categories=[\"pos\", \"neg\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1152fdd1-2aa0-4415-9bfc-a723d2db1ad9",
   "metadata": {},
   "source": [
    "train_data and test_data are dictionary-like objects with a few interesting attributes:\n",
    "\n",
    "* the attribute data returns a list of all text samples\n",
    "* the attribute target returns the labels (as integers)\n",
    "* the attribute target_names returns the names of the target classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2928974b-850e-406f-a23b-e47f7a25485d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data : Samples: 8000 - Labels: 8000 \n",
      "Test data     : Samples: 8000 - Labels: 8000 \n"
     ]
    }
   ],
   "source": [
    "#  Training data\n",
    "print(\n",
    "    \"Training data : Samples: {} - Labels: {} \".format(\n",
    "        len(train_data.data), len(train_data.target)\n",
    "    )\n",
    ")\n",
    "\n",
    "# Test data\n",
    "print(\n",
    "    \"Test data     : Samples: {} - Labels: {} \".format(\n",
    "        len(test_data.data), len(test_data.target)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4302c70b-393a-4ef4-9d76-9cf4f3b8f261",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neg', 'pos']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check target names \n",
    "train_data.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49606f1-fc88-41bc-b57c-131e011eb0c9",
   "metadata": {},
   "source": [
    "For negative reviews, the target value is 0; for positive reviews, it is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9b49302-f559-4a29-b482-cead45d70c92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print labels of first 3 samples\n",
    "train_data.target[0:3]  # [neg, neg, pos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2bbf766c-d3df-4e73-852f-ad9003f7a7ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Whatever possessed Guy Ritchie to remake Wertmuller\\'s film is incomprehensible.<br /><br />This new film is a mess. There was one other person in the audience when I saw it, and she left about an hour into it. (I hope she demanded a refund.) The only reason I stayed through to the end was because I\\'ve never walked out of a movie.<br /><br />But I sat through this piece of junk thoroughly flabbergasted that Madonna and Ritchie could actually think they made a good film. The dialogue is laughable, the acting is atrocious and the only nice thing in this film is the scenery. Ritchie took Lina\\'s movie and turned it into another \"Blue Lagoon.\"<br /><br />This is a film that you wouldn\\'t even waste time watching late night on Cinemax. Time is too precious to be wasted on crap like this.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print a review \n",
    "train_data.data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fac641ec-03cc-4a55-ae49-ff59e36693a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data:  (8000, 2)\n",
      "Test  data:     (8000, 2)\n"
     ]
    }
   ],
   "source": [
    "# Load data in a dataframe\n",
    "import pandas as pd\n",
    "\n",
    "# Training data\n",
    "df = pd.DataFrame({\"text\": train_data.data, \"label\": train_data.target})\n",
    "\n",
    "# Test data\n",
    "df_test = pd.DataFrame({\"text\": test_data.data, \"label\": test_data.target})\n",
    "\n",
    "print(\"Training data: \", df.shape)\n",
    "print(\"Test  data:    \", df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3ed088-4bff-4c04-9612-9a9be9af6b8b",
   "metadata": {},
   "source": [
    "Check for duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "529eb2a6-14fd-4445-aff0-b24dc9ac84c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nbr of duplicates in training data: 14\n",
      "Nbr of duplicates in test data:  38\n"
     ]
    }
   ],
   "source": [
    "print(\"Nbr of duplicates in training data: {}\".format(df.duplicated().sum()))\n",
    "print(\"Nbr of duplicates in test data:  {}\".format(df_test.duplicated().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1783d1ae-7893-47d4-a26b-aa0f1985a7c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data:  (7986, 2)\n",
      "Test  data:     (7962, 2)\n"
     ]
    }
   ],
   "source": [
    "# Drop duplicates from training/test data\n",
    "df.drop_duplicates(inplace=True)\n",
    "df_test.drop_duplicates(inplace=True)\n",
    "\n",
    "# Reset index\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(\"Training data: \", df.shape)\n",
    "print(\"Test  data:    \", df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c411060a-9306-429d-82f7-d68ebb6c2099",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>This is a movie that relies solely on the some...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>My friend's mom used to work at a video store ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>802</th>\n",
       "      <td>Looking at some of the negative posts, you rea...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>671</th>\n",
       "      <td>&lt;br /&gt;&lt;br /&gt;I saw this on the Sci-Fi channel. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6419</th>\n",
       "      <td>A very early Oliver Stone (associate-)produced...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  label\n",
       "889   This is a movie that relies solely on the some...      0\n",
       "261   My friend's mom used to work at a video store ...      0\n",
       "802   Looking at some of the negative posts, you rea...      1\n",
       "671   <br /><br />I saw this on the Sci-Fi channel. ...      0\n",
       "6419  A very early Oliver Stone (associate-)produced...      1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print a few samples\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c07f3e1a-82b3-467f-bd3c-2c3ecf0d8e8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3717</th>\n",
       "      <td>This demented left-wing wipe-out trivializes D...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3440</th>\n",
       "      <td>My father has worked with top secret informati...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7297</th>\n",
       "      <td>Let's face it.&lt;br /&gt;&lt;br /&gt;This movie is incred...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2652</th>\n",
       "      <td>There was a video out in America called 'Carto...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>This is really good. Original ideas in the fil...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  label\n",
       "3717  This demented left-wing wipe-out trivializes D...      0\n",
       "3440  My father has worked with top secret informati...      1\n",
       "7297  Let's face it.<br /><br />This movie is incred...      1\n",
       "2652  There was a video out in America called 'Carto...      1\n",
       "127   This is really good. Original ideas in the fil...      1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f86dcf-882e-4ffb-aa3b-46817e4e3a04",
   "metadata": {},
   "source": [
    "**Preliminary EDA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb51932c-e00b-42cb-bc36-b5c0e11e58fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    3995\n",
       "0    3991\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of samples per class in training data\n",
    "df[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d9a37258-7a54-486f-8dc2-db656fd7dd65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    3987\n",
       "1    3975\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of samples per class in test data\n",
    "df_test[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509b195f-44e1-401f-9c69-0377b2d062e7",
   "metadata": {},
   "source": [
    "We have approximately the same number of positive and negative reviews in the training and the test data. Therefore, we can assume that our dataset is balanced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d79533c-ea79-44de-ac27-ca13885c5877",
   "metadata": {},
   "source": [
    "*Text Preprocessing*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4953c3-e81b-4f4d-b9f6-67a44801b716",
   "metadata": {},
   "source": [
    "Below is a list of “issues” that we think we should address in this particular dataset.\n",
    "\n",
    "* Remove numbers and HTML code for line breaks\n",
    "* Convert text to lowercase.\n",
    "* Remove stop words.\n",
    "* Remove punctuation.\n",
    "* Apply stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "23cfedda-17ae-4b94-ab53-35a2fddd0ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text sample: \n",
      "I don't like the director's suit. But I LOVED his movie! 10/10 <br /><br />\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# remove numbers and HTML breaks as for this task of finding neg/pos not relevant\n",
    "sample = \"I don't like the director's suit. But I LOVED his movie! 10/10 <br /><br />\"\n",
    "\n",
    "print(\"Text sample: \\n{}\\n\".format(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a24540f4-2839-46f1-944c-607b8bdcb906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove html code for line breaks: \n",
      "I don't like the director's suit. But I LOVED his movie! 10/10 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "\n",
    "# remove html code\n",
    "clean_sample = re.sub(r\"<br />\", \"\", sample)\n",
    "\n",
    "print(\"Remove html code for line breaks: \\n{}\\n\".format(clean_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d95e724d-2152-4584-8c4c-23ba3b2010d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove numbers from sample: \n",
      "I don't like the director's suit. But I LOVED his movie! / \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove numbers\n",
    "clean_sample = re.sub(r\"\\d+\", \"\", clean_sample)\n",
    "\n",
    "print(\"Remove numbers from sample: \\n{}\\n\".format(clean_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44f975d-76cc-4883-954e-39bc4dfb2e9f",
   "metadata": {},
   "source": [
    "*Case folding*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1774b5ff-7a71-4372-9018-90ea264d95dc",
   "metadata": {},
   "source": [
    "Number of unique words in a dataset usually called vocabulary. Reducing this benefit of reducing computational resource required to train ML model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ea20805-f1b8-4cf0-ab73-6168256ce73e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convert to lowercase: \n",
      "i don't like the director's suit. but i loved his movie! / \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Case folding\n",
    "clean_sample = clean_sample.lower()\n",
    "\n",
    "print(\"Convert to lowercase: \\n{}\\n\".format(clean_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63bec5bd-aa2c-4361-bc53-fc79a9fb9227",
   "metadata": {},
   "source": [
    "*Tokenization*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70433d47-1f40-48d2-b6c1-f8829acae559",
   "metadata": {},
   "source": [
    "Process of splitting text into smaller segments called tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "50ffce19-95b6-4aca-911f-8d901bdbc940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', \"don't\", 'like', 'the', \"director's\", 'suit.', 'but', 'i', 'loved', 'his', 'movie!', '/']\n"
     ]
    }
   ],
   "source": [
    "print(clean_sample.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d42aea-fb8c-42be-9721-095940c218ef",
   "metadata": {},
   "source": [
    "However, this tokenizer is a bit “naive”. For example:\n",
    "\n",
    "* It didn’t separate punctuation from the adjacent words (e.g. “suit.”, “movie!”).\n",
    "* It didn’t separate the token “‘s” from the word “director”.\n",
    "* It didn’t expand the contraction “don’t” to its constituent words “do” and “n’t”."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcff6025-1671-4e7b-bf00-1444b1d7d240",
   "metadata": {},
   "source": [
    "*TreebankWordTozenizer* more sophisticated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e51e6b-dd1c-4eef-814e-3faf09f3a2b1",
   "metadata": {},
   "source": [
    "TreebankWordTokenizer incorporates common rules for English grammar and offers a few nice options. For example:\n",
    "\n",
    "* It separates most punctuation characters from adjacent words.\n",
    "* It splits standard contractions, e.g. don’t -> do   n’t and they’ll -> they   ‘ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f686dcfb-2bbb-4c69-b792-74ecb653f3f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before tokenization: \n",
      "i don't like the director's suit. but i loved his movie! / \n",
      "\n",
      "After tokenization: \n",
      "['i', 'do', \"n't\", 'like', 'the', 'director', \"'s\", 'suit.', 'but', 'i', 'loved', 'his', 'movie', '!', '/']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import tokenizer\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "# Create an instance of the TreebankWordTokenizer object\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "print(\"Before tokenization: \\n{}\\n\".format(clean_sample))\n",
    "print(\"After tokenization: \\n{}\\n\".format(tokenizer.tokenize(clean_sample)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155bece1-1581-4f04-a86e-abadab8fc817",
   "metadata": {},
   "source": [
    "TreebankWordTokenizer assumes that the text has already been segmented into sentences (our text sample has two sentences). For this reason, we need to first split the text into sentences using the nltk.tokenize.sent_tokenize() method and then tokenize each sentence with the TreebankWordTokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "910089c5-d0dd-402c-9b10-cb1298d24d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before tokenization: \n",
      "i don't like the director's suit. but i loved his movie! / \n",
      "\n",
      "After tokenization: \n",
      "['i', 'do', \"n't\", 'like', 'the', 'director', \"'s\", 'suit', '.', 'but', 'i', 'loved', 'his', 'movie', '!', '/']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "nltk.download(\"punkt\", quiet=True)\n",
    "\n",
    "\n",
    "def review_tokenizer(text):\n",
    "    \"\"\"\n",
    "    Input: a sentence, or sentences\n",
    "    Ouput: list of tokens\n",
    "\n",
    "    \"\"\"\n",
    "    # Create empty list\n",
    "    words = []\n",
    "\n",
    "    # Split into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    for sent in sentences:\n",
    "        words.extend(tokenizer.tokenize(sent))\n",
    "    return words\n",
    "\n",
    "\n",
    "print(\"Before tokenization: \\n{}\\n\".format(clean_sample))\n",
    "print(\"After tokenization: \\n{}\\n\".format(review_tokenizer(clean_sample)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a137e5-4c4d-48ad-8cb5-7bdd936227b2",
   "metadata": {},
   "source": [
    "*Stop words*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a231035d-ba21-4655-bc7a-388f5fedb5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load english stopwords from nltk\n",
    "\n",
    "nltk.download(\"stopwords\", quiet=True)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_english = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "69f0d9a5-7cc1-42c7-8d25-74deba6a6755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179 stopwords in NLTK's list.\n",
      "\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "# Print stopwords\n",
    "\n",
    "print(\"{} stopwords in NLTK's list.\\n\".format(len(stopwords_english)))\n",
    "print(stopwords_english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f626f119-1216-4a24-89bd-7755a02f5824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before removing stop words: \n",
      "i don't like the director's suit. but i loved his movie! / \n",
      "\n",
      "[\"n't\", 'like', 'director', \"'s\", 'suit', '.', 'loved', 'movie', '!', '/']\n"
     ]
    }
   ],
   "source": [
    "print(\"Before removing stop words: \\n{}\\n\".format(clean_sample))\n",
    "\n",
    "# Use a list comprehension to remove stopwords from clean_sample\n",
    "print([tok for tok in review_tokenizer(clean_sample) if tok not in stopwords_english])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cc974632-e43c-469f-8a25-06245158bdf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before removing stop words: \n",
      "i don't like the director's suit. but i loved his movie! / \n",
      "\n",
      "After removing stop words: \n",
      "n't like director 's suit . loved movie ! /\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Before removing stop words: \\n{}\\n\".format(clean_sample))\n",
    "\n",
    "clean_sample = \" \".join(\n",
    "    [tok for tok in review_tokenizer(clean_sample) if tok not in stopwords_english]\n",
    ")\n",
    "print(\"After removing stop words: \\n{}\\n\".format(clean_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3989ee0-9850-4f50-9a9c-48cddf72e50f",
   "metadata": {},
   "source": [
    "*Modifying stop words lists*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8d7350c2-e9bd-427d-8e0c-e50905d31981",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if 'not' is in stopwords_english\n",
    "\n",
    "\"not\" in stopwords_english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3e73bd16-bec4-46da-bfd1-ce49ffff2a28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if \"no\" is in stopwords_english\n",
    "\n",
    "\"no\" in stopwords_english"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4373ac1d-914d-4a1a-ab7c-25f5001157d3",
   "metadata": {},
   "source": [
    "Negation words such as: “not”, “no”, and “nor” are part of NLTK’s stop word list. However, removing negation words from the text may be undesirable for some tasks, such as sentiment analysis. In the next cell, we show how to remove words from stop words lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b1f341aa-1ce2-4126-8f58-4676642128b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify nltk's stop word list: Remove negation words\n",
    "\n",
    "not_stopwords = {\"no\", \"nor\", \"not\"}\n",
    "\n",
    "custom_stopwords = set(\n",
    "    [word for word in stopwords_english if word not in not_stopwords]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca108e61-e4a6-42e4-90ce-a6c6b8f693f4",
   "metadata": {},
   "source": [
    "Can also add words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4fb69cb0-116e-4842-9f61-14cade35e96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify nltk's stop word list: Add tokens \"'s'\" and \"'m\"\n",
    "\n",
    "custom_stopwords = custom_stopwords.union({\"'s\", \"'m\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ed0ef424-0599-472a-8943-0bfd23d1b0a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After removing stop words using our custom stopwords list: \n",
      "n't like director suit . loved movie ! /\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clean_sample = \" \".join(\n",
    "    [tok for tok in review_tokenizer(clean_sample) if tok not in custom_stopwords]\n",
    ")\n",
    "print(\n",
    "    \"After removing stop words using our custom stopwords list: \\n{}\\n\".format(\n",
    "        clean_sample\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9e4dc0-1273-4f9b-8797-8716805892c3",
   "metadata": {},
   "source": [
    "*Punctuation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "57eff20a-a915-4933-a2ae-0647b2c681d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "# string.punctuation returns all punctuation characters in a string\n",
    "print(type(string.punctuation))\n",
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a26b43ed-41ba-4f12-98ff-404714ba657e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"n't like director suit loved movie\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use a list comprehension to remove punctuation characters from clean_sample\n",
    "\" \".join(\n",
    "    [tok for tok in review_tokenizer(clean_sample) if tok not in string.punctuation]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0226ec11-eb2d-4094-91e1-15bf2822b9e4",
   "metadata": {},
   "source": [
    "*Stemming*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e184fd-597e-4c24-97ab-be61eb5b2c24",
   "metadata": {},
   "source": [
    "process of converting a word to its “root” or “stem”, mainly by stripping suffixes from the end of a word.\n",
    "\n",
    "TEXT\tSTEM\n",
    "loved\tlove\n",
    "love\tlove\n",
    "movies\tmovi\n",
    "movie\tmovi\n",
    "actors\tactor\n",
    "acting\tact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7e51c925-c325-4206-a1db-2f50ade39f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load stemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# Create an instance of the PorterStemmer object\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f0f7d3d8-36dc-4fc5-af65-56bf1b221a45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'love'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stem of \"loved\"\n",
    "stemmer.stem(\"loved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "edbd32e0-1831-4ec4-976b-69201bcc59f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw sample: \n",
      "I don't like the director's suit. But I LOVED his movie! 10/10 <br /><br />\n",
      "\n",
      "Remove line breaks and numbers: \n",
      "I don't like the director's suit. But I LOVED his movie! /   \n",
      "\n",
      "Convert to lowercase: \n",
      "i don't like the director's suit. but i loved his movie! /   \n",
      "\n",
      "Remove stopwords & punctuation and stem tokens: \n",
      "n't like director suit love movi\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Raw sample: \\n{}\\n\".format(sample))\n",
    "\n",
    "# Remove html code for line breaks\n",
    "clean_sample = re.sub(r\"<br />\", \" \", sample)\n",
    "\n",
    "# Remove numbers\n",
    "clean_sample = re.sub(r\"\\d+\", \"\", clean_sample)\n",
    "\n",
    "print(\"Remove line breaks and numbers: \\n{}\\n\".format(clean_sample))\n",
    "\n",
    "# Case folding\n",
    "clean_sample = clean_sample.lower()\n",
    "print(\"Convert to lowercase: \\n{}\\n\".format(clean_sample))\n",
    "\n",
    "# Remove stopwords & punctuation and stem tokens\n",
    "clean_sample = \" \".join(\n",
    "    [\n",
    "        stemmer.stem(word)\n",
    "        for word in review_tokenizer(clean_sample)\n",
    "        if (word not in string.punctuation) and (word not in custom_stopwords)\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Remove stopwords & punctuation and stem tokens: \\n{}\\n\".format(clean_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7f7609-3fab-4829-827d-cc7183eea2b2",
   "metadata": {},
   "source": [
    "Stemming vs. Lemmatization\n",
    "\n",
    "In the previous cells, we used stemming to connect similar words and reduce vocabulary size. However, stemming can generate “non-words”, i.e., words you won’t find in a dictionary. If you need your preprocessed text to be “readable”, then you may prefer lemmatization. Lemmatization is the process of converting words to their “dictionary” form (lemma). It is used for keyword extraction and topic analysis tasks, as it always produces interpretable “terms”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "deb6f8c9-193a-4dc2-90c4-dcf2c1fa73cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_review(text):\n",
    "\n",
    "    \"\"\"\n",
    "    Preprocess raw samples of text.\n",
    "\n",
    "    INPUT:\n",
    "    - text: raw text (string)\n",
    "\n",
    "    OUTPUT:\n",
    "    - clean_text: processed text (string)\n",
    "    \"\"\"\n",
    "\n",
    "    clean_text = text\n",
    "\n",
    "    # Case folding\n",
    "    clean_text = clean_text.lower()\n",
    "\n",
    "    # Remove html code for line breaks\n",
    "    clean_text = re.sub(r\"<br />\", \" \", clean_text)\n",
    "\n",
    "    # Remove numbers\n",
    "    clean_text = re.sub(r\"\\d+\", \"\", clean_text)\n",
    "    \n",
    "    # Strip periods (otherwise, sent_tokenize throws an error)\n",
    "    clean_text = clean_text.strip('\\. ') \n",
    "\n",
    "    # Tokenize review\n",
    "    tokens = review_tokenizer(clean_text)\n",
    "\n",
    "    # Remove punctuation, stop words, single char tokens and stem tokens\n",
    "    clean_text = \" \".join(\n",
    "        [\n",
    "            stemmer.stem(tok)\n",
    "            for tok in tokens\n",
    "            if (tok not in string.punctuation)\n",
    "            and (tok not in custom_stopwords)\n",
    "            and (len(tok) > 1)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Remove multiple punctuation characters (otherwise not removed)\n",
    "    clean_text = re.sub(r\"[\\'*_`\\.-]{2,}\", \" \", clean_text)\n",
    "\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "70a4afb5-ef08-44a1-b10a-34f3b279fcae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before cleaning: \n",
      "Wenders was great with Million $ Hotel.I don't know how he came up with this film! The idea of giving the situation after spt11 and the view of American Society is hopeful,that makes it 2 out of ten.But this is not a movie.Is that the best someone can do with a great idea(the west-east clash).There are important things going on in middle east and it is just issued on the screen of a MAC* with the fingers of an Amerian girl who is actually at the level of stupidity(because she is just ignorant about the facts).The characters are not well shaped.And the most important thing is the idea that is given with religion is somehow funny to me.At the ending scene Lana says lets just be quiet and try to listen.And the background music says \"...I will pray\".The thing is not about religion actually.But it ends up with this.How you are gonna see the truth if you just close your eyes and pray.The lights are already shining on the truth.Its just that nobody wants to see it. ps: \"My home is not a place.It is people\"The only thing that gets 10 out of 10 is that sentence.But it is wasted behind this film making. (by the way; as \"someone\" mentioned below ,Americas finest young man are not finest,they are just the \"poor\" and the \"hopeless\" ones who sign up for the army in need of good paychecks which is not provided by the government ! )\n",
      "\n",
      "After cleaning: \n",
      "wender great million hotel.i n't know came film idea give situat spt view american societi hope make ten.but not movie.i best someon great idea west-east clash .there import thing go middl east issu screen mac* finger amerian girl actual level stupid ignor fact .the charact not well shaped.and import thing idea given religion somehow funni me.at end scene lana say let quiet tri listen.and background music say     pray   .the thing not religion actually.but end this.how gon na see truth close eye pray.th light alreadi shine truth.it nobodi want see ps   home not place.it peopl   thing get sentence.but wast behind film make way   someon   mention america finest young man not finest   poor     hopeless   one sign armi need good paycheck not provid govern\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample a review randomly from the training data\n",
    "import random\n",
    "\n",
    "sample = df.iloc[random.randint(0, len(df) - 1), 0]\n",
    "\n",
    "print(\"Before cleaning: \\n{}\\n\".format(sample))\n",
    "print(\"After cleaning: \\n{}\\n\".format(preprocess_review(sample)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2a1142-098c-4c8c-85e5-0f125fe50be3",
   "metadata": {},
   "source": [
    "Now apply to training/test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "17307626-76ef-449a-9175-279302b974a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess training data\n",
    "df[\"clean_text\"] = df[\"text\"].apply(preprocess_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "508e7710-ef6f-4346-8e2f-d7bd41b8ae4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess test data\n",
    "df_test[\"clean_text\"] = df_test[\"text\"].apply(preprocess_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "52bcdbc1-099e-4e0c-b3ec-bb2a200de64c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data:  (7985, 3)\n",
      "Test data:      (7962, 3)\n"
     ]
    }
   ],
   "source": [
    "# Re-order columns\n",
    "df = df[[\"text\", \"clean_text\", \"label\"]].copy()\n",
    "df_test = df_test[[\"text\", \"clean_text\", \"label\"]].copy()\n",
    "\n",
    "# Drop duplicates\n",
    "# Good practice: re-check for duplicates after cleaning\n",
    "df = df.drop_duplicates(\"clean_text\")\n",
    "df_test = df_test.drop_duplicates(\"clean_text\")\n",
    "\n",
    "# Reset index\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "df_test.reset_index(inplace=True, drop=True)\n",
    "\n",
    "print(\"Training data: \", df.shape)\n",
    "print(\"Test data:     \", df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2deda7f3-03fa-4f4e-a287-b057c4e71388",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2859</th>\n",
       "      <td>I have seen this film many times and I like al...</td>\n",
       "      <td>seen film mani time like bad teacher want give...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  \\\n",
       "2859  I have seen this film many times and I like al...   \n",
       "\n",
       "                                             clean_text  label  \n",
       "2859  seen film mani time like bad teacher want give...      1  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train data\n",
    "df.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "99e3e837-c7c8-4ae2-8ace-f8d2c3f142b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6705</th>\n",
       "      <td>Hey, remember when Hal Hartley was brilliant? ...</td>\n",
       "      <td>hey rememb hal hartley brilliant time 'd say b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  \\\n",
       "6705  Hey, remember when Hal Hartley was brilliant? ...   \n",
       "\n",
       "                                             clean_text  label  \n",
       "6705  hey rememb hal hartley brilliant time 'd say b...      0  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test data\n",
    "df_test.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da616b51-e817-4e53-85d8-3f50681fa6b6",
   "metadata": {},
   "source": [
    "Save preprocessed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "83536650-2e4c-4ad5-b108-cf3715ce080d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export clean data\n",
    "df.to_pickle(\"c4_imdb-8k_train.pkl\")\n",
    "df_test.to_pickle(\"c4_imdb-8k_test.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac148d46-ccad-41ab-b701-6db9f4ff6716",
   "metadata": {},
   "source": [
    "**Feature Extraction & Modelling**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640d3bab-6048-4f98-8f08-31adc4467a1d",
   "metadata": {},
   "source": [
    "Reload all data again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "edb8bc77-0764-4435-a9c4-7b13f2c78460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d9bc3a64-b38e-4177-801b-7fe70c2a7240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cleaned data\n",
    "import pickle\n",
    "\n",
    "with open(\"c4_imdb-8k_train.pkl\", \"rb\") as f:\n",
    "    df = pickle.load(f)\n",
    "\n",
    "with open(\"c4_imdb-8k_test.pkl\", \"rb\") as f:\n",
    "    df_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bb5d3a89-f71c-4746-a79f-8beb71555e69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1482</th>\n",
       "      <td>Here's an oddity: a modern-day film about a se...</td>\n",
       "      <td>odditi modern-day film serial killer no fight ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>924</th>\n",
       "      <td>Back in my days as an usher \"Private Lessons\" ...</td>\n",
       "      <td>back day usher   privat lesson   play -plex wo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  \\\n",
       "1482  Here's an oddity: a modern-day film about a se...   \n",
       "924   Back in my days as an usher \"Private Lessons\" ...   \n",
       "\n",
       "                                             clean_text  label  \n",
       "1482  odditi modern-day film serial killer no fight ...      1  \n",
       "924   back day usher   privat lesson   play -plex wo...      0  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fabb8a-c0c1-46ba-bc94-aa25573ef160",
   "metadata": {},
   "source": [
    "**Feature Extraction**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58030cb0-dd5c-4930-bc36-7c6a92ef934c",
   "metadata": {},
   "source": [
    "*Bag of Words*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695ff2d1-333a-4942-90ef-a99fe3c13524",
   "metadata": {},
   "source": [
    "The main idea behind the Bag of words method is to represent each text sample by a list of word counts, without keeping any information about the order of words in texts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8817ca21-655c-4865-a34e-3f26f96b9651",
   "metadata": {},
   "source": [
    "Datasets of text are often called corpora, and text samples are called documents; these are two terms that you will often encounter when working with text data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6ad6affc-2ee0-4dcc-b41b-ea9280f44aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toy corpus:\n",
      "\n",
      "['Scikit-learn makes ML easy, easy as 123', 'Learning TensorFlow for deep learning']\n"
     ]
    }
   ],
   "source": [
    "# Toy corpus\n",
    "X_toy = [\n",
    "    \"Scikit-learn makes ML easy, easy as 123\",\n",
    "    \"Learning TensorFlow for deep learning\",\n",
    "]\n",
    "\n",
    "# Show toy corpus\n",
    "print(\"Toy corpus:\\n\")\n",
    "print(list(X_toy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f73ba5-62b4-49c6-b973-4dfeac91315a",
   "metadata": {},
   "source": [
    "We can implement the Bag of words model with the CountVectorizer object from scikit-learn. Below we import the Bag of words model and create an instance of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "947f3717-ab5c-4524-a509-5157a18d0c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create an instance of CountVectorizer\n",
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "95a0ed97-cd9c-43d0-96aa-958270601838",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit Bag of words (BoW) on toy corpus\n",
    "vectorizer.fit(X_toy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45106151-40b7-4725-8973-fbd8ff99533c",
   "metadata": {},
   "source": [
    "First, the CountVectorizer model tokenizes all texts in the corpus and builds the vocabulary (i.e., the list of unique words in the corpus). We can check which words are in the vocabulary using the get_feature_names_out method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2e18c162-381e-4a85-baba-b5450a05a705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary length: 11\n",
      "\n",
      "['123' 'as' 'deep' 'easy' 'for' 'learn' 'learning' 'makes' 'ml' 'scikit'\n",
      " 'tensorflow']\n"
     ]
    }
   ],
   "source": [
    "# Show vocabulary\n",
    "vocabulary = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Print vocabulary length\n",
    "print(\"Vocabulary length: {}\\n\".format(len(vocabulary)))\n",
    "\n",
    "# Show words in the vocabulary\n",
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524d4349-eb60-4e8e-82e5-503af8e859c1",
   "metadata": {},
   "source": [
    "Once the texts are tokenized, and the vocabulary is established, Bag of Words counts how many times each word appears in each text from the corpus. We call the transform method to transform each text sample into its Bag of words representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2cb83232-8ae1-406b-937d-64bd0a6657c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 11)\n"
     ]
    }
   ],
   "source": [
    "# Get Bag of words representation of toy data\n",
    "X_bow_toy = vectorizer.transform(X_toy)\n",
    "\n",
    "print(X_bow_toy.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2099ce-47f9-484a-b7a7-2fa91b3be2b7",
   "metadata": {},
   "source": [
    "The shape of X_bow_toy is (2,11). Each row corresponds to a text from the corpus and each column to a word in the vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893b5ec1-b98f-4d56-9763-8e8016509516",
   "metadata": {},
   "source": [
    "Scikit-learn stores only non-zero values in “sparse matrices”, which is more memory efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7b761f0c-4f0f-4526-8e46-509c5dade9ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2x11 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 11 stored elements in Compressed Sparse Row format>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Scikit-learn uses sparse matrices instead of Numpy arrays\n",
    "display(X_bow_toy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b862d7cd-7064-492b-93ce-b5975be95176",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 2, 0, 1, 0, 1, 1, 1, 0],\n",
       "       [0, 0, 1, 0, 1, 0, 2, 0, 0, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_bow_toy.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cd9acae8-3639-4293-a92e-55512ca22dce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>123</th>\n",
       "      <th>as</th>\n",
       "      <th>deep</th>\n",
       "      <th>easy</th>\n",
       "      <th>for</th>\n",
       "      <th>learn</th>\n",
       "      <th>learning</th>\n",
       "      <th>makes</th>\n",
       "      <th>ml</th>\n",
       "      <th>scikit</th>\n",
       "      <th>tensorflow</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Text</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Scikit-learn makes ML easy, easy as 123</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Learning TensorFlow for deep learning</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         123  as  deep  easy  for  learn  \\\n",
       "Text                                                                       \n",
       "Scikit-learn makes ML easy, easy as 123    1   1     0     2    0      1   \n",
       "Learning TensorFlow for deep learning      0   0     1     0    1      0   \n",
       "\n",
       "                                         learning  makes  ml  scikit  \\\n",
       "Text                                                                   \n",
       "Scikit-learn makes ML easy, easy as 123         0      1   1       1   \n",
       "Learning TensorFlow for deep learning           2      0   0       0   \n",
       "\n",
       "                                         tensorflow  \n",
       "Text                                                 \n",
       "Scikit-learn makes ML easy, easy as 123           0  \n",
       "Learning TensorFlow for deep learning             1  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_BOW_toy = pd.DataFrame(\n",
    "    X_bow_toy.toarray(), columns=vectorizer.get_feature_names_out()\n",
    ")\n",
    "df_BOW_toy[\"Text\"] = X_toy\n",
    "df_BOW_toy.set_index(\"Text\", inplace=True)\n",
    "\n",
    "df_BOW_toy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d756d7b9-70f5-4572-b97d-62a89e24a727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create arrays of texts from training/test data\n",
    "# (scikit-learn models work better with np.arrays)\n",
    "\n",
    "X_train = df[\"clean_text\"].values\n",
    "X_test = df_test[\"clean_text\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b625cf7d-713c-4e87-bc65-6edc5b28b4dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of words vectors\n",
      "Training data:  (7985, 31906)\n",
      "Test data:      (7962, 31906)\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Apply fit_transform method to training data\n",
    "X_bow = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform test data\n",
    "X_bow_te = vectorizer.transform(X_test)\n",
    "\n",
    "print(\"Bag of words vectors\")\n",
    "print(\"Training data:  {}\".format(X_bow.shape))\n",
    "print(\"Test data:      {}\".format(X_bow_te.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0941ffe-a722-43ef-837a-357bfa4f2e91",
   "metadata": {},
   "source": [
    "The fit_transform method will first fit the CountVectorizer object and build the vocabulary based on the training data. Then, the transform method will convert the training data to lists of word counts.\n",
    "\n",
    "Note that we should only use the transform method with the test data; this will transform the test data into lists of word counts based on the vocabulary built from the training data. Any words from the test data that are not part of the vocabulary will be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "eeaad924-a28f-46e7-b813-385451e23ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 31906\n",
      "\n",
      "Show some words from the vocabulary:\n",
      "\n",
      "['_a' 'abandon' 'abraham' 'acapulco' 'ackland' 'actually' 'admit'\n",
      " 'adventuresom' 'afghanistan' 'aghast' 'aicn' 'airwolf' 'alban' 'alfonso'\n",
      " 'alliances' 'already' 'amass' 'americanis' 'amrutl' 'ancona' 'angles'\n",
      " 'anne' 'anthropolog' 'any' 'apolit' 'apposit' 'arcan' 'arhtur' 'army'\n",
      " 'artifact' 'ashura' 'assum' 'athen' 'attorney' 'aureliu' 'autumn'\n",
      " 'awakeningli' 'baaad' 'backlot' 'bagman' 'balkan' 'bandera' 'barbecu'\n",
      " 'baro' 'basicli' 'baubl' 'bean' 'becom' 'beforehand' 'belgrad' 'bene'\n",
      " 'bergenon' 'bessi' 'bhang' 'bigtim' 'bipartisan' 'bizarro' 'blalock'\n",
      " 'blemish' 'blondel' 'blueprint' 'bodaci' 'bolden' 'bonk' 'boot'\n",
      " 'boskovich' 'bouyant' 'brag' 'brauss' 'bregana' 'brigadoon' 'broaden'\n",
      " 'brought' 'btw' 'buford' 'bullwinkl' 'burgundi' 'businessmen' 'byrn'\n",
      " 'cafeteria' 'call' 'camilla' 'cann' 'caprioli' 'carey' 'carpent'\n",
      " 'casablanka' 'caswel' 'catscratch' 'ccthemovieman' 'cena' 'chaco'\n",
      " 'chandra' 'charisma' 'chazen' 'cheetor' 'cheyenn' 'chimpanze' 'chopra'\n",
      " 'chu']\n"
     ]
    }
   ],
   "source": [
    "# Show vocabulary\n",
    "vocab = vectorizer.get_feature_names_out()\n",
    "\n",
    "print(\"Vocabulary size: {}\\n\".format(len(vocab)))\n",
    "\n",
    "print(\"Show some words from the vocabulary:\\n\")\n",
    "print(vocab[0:5000:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "432d2ce1-ec64-47ba-995f-2b94682c1a9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_a</th>\n",
       "      <th>_any_</th>\n",
       "      <th>_apocalypt</th>\n",
       "      <th>_atlanti</th>\n",
       "      <th>_atlantis_</th>\n",
       "      <th>_both_</th>\n",
       "      <th>_could</th>\n",
       "      <th>_discuss_</th>\n",
       "      <th>_discussing_</th>\n",
       "      <th>_earned_</th>\n",
       "      <th>...</th>\n",
       "      <th>zuzz</th>\n",
       "      <th>zvezda</th>\n",
       "      <th>zwartboek</th>\n",
       "      <th>zwick</th>\n",
       "      <th>zz</th>\n",
       "      <th>zzzz</th>\n",
       "      <th>zzzzzzzz</th>\n",
       "      <th>zã</th>\n",
       "      <th>â½</th>\n",
       "      <th>ã¼ber</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>doc_1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc_2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc_3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc_4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc_5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31906 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       _a  _any_  _apocalypt  _atlanti  _atlantis_  _both_  _could  _discuss_  \\\n",
       "doc_1   0      0           0         0           0       0       0          0   \n",
       "doc_2   0      0           0         0           0       0       0          0   \n",
       "doc_3   0      0           0         0           0       0       0          0   \n",
       "doc_4   0      0           0         0           0       0       0          0   \n",
       "doc_5   0      0           0         0           0       0       0          0   \n",
       "\n",
       "       _discussing_  _earned_  ...  zuzz  zvezda  zwartboek  zwick  zz  zzzz  \\\n",
       "doc_1             0         0  ...     0       0          0      0   0     0   \n",
       "doc_2             0         0  ...     0       0          0      0   0     0   \n",
       "doc_3             0         0  ...     0       0          0      0   0     0   \n",
       "doc_4             0         0  ...     0       0          0      0   0     0   \n",
       "doc_5             0         0  ...     0       0          0      0   0     0   \n",
       "\n",
       "       zzzzzzzz  zã  â½  ã¼ber  \n",
       "doc_1         0   0   0      0  \n",
       "doc_2         0   0   0      0  \n",
       "doc_3         0   0   0      0  \n",
       "doc_4         0   0   0      0  \n",
       "doc_5         0   0   0      0  \n",
       "\n",
       "[5 rows x 31906 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_BOW = pd.DataFrame(X_bow.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "df_BOW.index = [\"doc_\" + str(i + 1) for i in range(0, len(df_BOW))]\n",
    "df_BOW.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab95bfb2-55bb-47a5-93f9-8c76c763fd4e",
   "metadata": {},
   "source": [
    "Bag of words has encoded each document from the corpus into a list of word counts.\n",
    "\n",
    "* Each column represents a word in the vocabulary. Each row represents a document in the corpus.\n",
    "* The length of lists of word counts is equal to the size of the vocabulary.\n",
    "* Bag of words generates lists of word counts that are long (typical sizes are: 10K - 1M) and sparse (i.e., most values are zero)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7d8307-492e-42e5-8d6d-05897fe4fa5f",
   "metadata": {},
   "source": [
    "*Reducing vocabulary size*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c3494d-4d14-4096-8e6f-e7d2a8639e74",
   "metadata": {},
   "source": [
    "* *min_df*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d219cb47-1774-4093-a16f-cdff898e7a82",
   "metadata": {},
   "source": [
    "Used to ignore rare terms in the corpus when building the vocabulary. For example, in the next cell, a min_df of 2 means including only words that appear in more than two documents in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5ef2d91d-a055-4da6-b147-592184cf2224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of words vectors:      (7985, 18603)\n"
     ]
    }
   ],
   "source": [
    "# min_df : ignore rare terms\n",
    "\n",
    "# Create an instance of CountVectorizer\n",
    "vectorizer = CountVectorizer(\n",
    "    min_df=2\n",
    ")  # ignore terms that appear in less than 2 reviews\n",
    "\n",
    "# Apply BoW to corpus\n",
    "X_bow = vectorizer.fit_transform(X_train)\n",
    "\n",
    "print(\"Bag of words vectors:      {}\".format(X_bow.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08683717-2298-4814-9a9a-7efb344d2ec6",
   "metadata": {},
   "source": [
    "* *ngram_range*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedfc2dc-b97d-46c2-b663-4fccc003013f",
   "metadata": {},
   "source": [
    "The “meaning” of a text may be lying in combinations of words, not just single words. Including longer n-grams, such as pairs or triplets of words appearing next to each other in a sentence, can help to incorporate more information about the content of a text into our models. Single words are also called unigrams, pairs of words bi-grams, and word triplets tri-grams."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23871962-84e5-43d5-ad91-7255188e007a",
   "metadata": {},
   "source": [
    "Below, we create an instance of CountVectorizer where we use ngram_range=(1, 2) to include unigrams and bi-grams in the vocabulary. We also use min_df=2 to ignore rare words appearing in less than two documents in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "69a0b52d-2fa4-4207-b594-4cb4f2590908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of words vectors:      (7985, 121507)\n",
      "\n",
      "Show some words from the vocabulary:\n",
      "\n",
      "['_the' 'aavjo' 'abandon idea' 'abbi' 'abe' 'abhorr' 'abil film'\n",
      " 'abil wish' 'abl complet' 'abl experi' 'abl identifi' 'abl pull'\n",
      " 'abl spot' 'abl use' 'abod' 'above averag' 'absenc' 'absolut absent'\n",
      " 'absolut brilliant' 'absolut fantast' 'absolut horrif' 'absolut not'\n",
      " 'absolut silli' 'absolut worst' 'absurd charact']\n"
     ]
    }
   ],
   "source": [
    "# ngram_range : include longer ngrams in the vocabulary\n",
    "\n",
    "# Create an instance of CountVectorizer\n",
    "vectorizer = CountVectorizer(\n",
    "    min_df=2,  # ignore words that appear in less than 2 reviews\n",
    "    ngram_range=(1, 2),  # include unigrams and bi-grams in the vocabulary\n",
    ")\n",
    "\n",
    "# Apply BoW to corpus\n",
    "X_bow = vectorizer.fit_transform(X_train)\n",
    "\n",
    "print(\"Bag of words vectors:      {}\\n\".format(X_bow.shape))\n",
    "\n",
    "# Lets have a look at the vocabulary\n",
    "vocab = vectorizer.get_feature_names_out()\n",
    "\n",
    "print(\"Show some words from the vocabulary:\\n\")\n",
    "print(vocab[0:250:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7671098c-7761-4366-ab23-246b28eeafd2",
   "metadata": {},
   "source": [
    "* *max_features*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3488e3a8-153c-449d-b454-0d838a7d03b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of words vectors:      (7985, 15000)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    min_df=2,  # ignore words that appear in less than 2 reviews\n",
    "    ngram_range=(1, 2),  # consider unigrams and bigrams in the vocabulary\n",
    "    max_features=15000,  # use only top 15'000 most frequent terms in vocabulary\n",
    ")\n",
    "\n",
    "# Apply BoW to corpus\n",
    "X_bow = vectorizer.fit_transform(X_train)\n",
    "\n",
    "print(\"Bag of words vectors:      {}\\n\".format(X_bow.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "359176cc-4def-414f-998c-b76ce692b2e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Show some words from the vocabulary:\n",
      "\n",
      "['abandon' 'abbey' 'abbi' 'abbot' 'abc' 'abduct' 'abhay' 'abhay deol'\n",
      " 'abil' 'abl' 'abl get' 'abl make' 'abl see' 'abl watch' 'aboard' 'abomin'\n",
      " 'abort' 'abound' 'abraham' 'abroad' 'abrupt' 'abruptli' 'absenc' 'absent'\n",
      " 'absolut' 'absolut hilari' 'absolut love' 'absolut no' 'absolut noth'\n",
      " 'absolut worst' 'absorb' 'abstract' 'absurd' 'abu' 'abund' 'abus' 'abysm'\n",
      " 'academ' 'academi' 'academi award' 'accent' 'accentu' 'accept' 'access'\n",
      " 'accid' 'accident' 'acclaim' 'accolad' 'accommod' 'accompani']\n"
     ]
    }
   ],
   "source": [
    "# Lets have a look at the vocabulary\n",
    "vocab = vectorizer.get_feature_names_out()\n",
    "\n",
    "print(\"Show some words from the vocabulary:\\n\")\n",
    "print(vocab[0:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f796002-3aa1-4c05-a4cc-23b43f1e6862",
   "metadata": {},
   "source": [
    "As expected, terms such as movi, film, charact, watch, scene, director, etc., appear in the topmost frequent terms in the training data. We can also see terms expressing “sentiment” such as: not, like, good, no, great, love, etc, among the topmost frequent terms. These terms are more likely to be informative about the “sentiment” of a movie review."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77541f36-06f0-4d36-9ae1-b0d6b6ead5bc",
   "metadata": {},
   "source": [
    "*In the cells above, we discussed how min_df, max_features, and ngram_range affect term selection in the vocabulary and vocabulary size. Note that all of them are hyper-parameters of the CountVectorizer model, and we can tune them together with a machine learning model to optimize model performance.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c59e82-3896-44f9-8386-47e8d7a9d2a7",
   "metadata": {},
   "source": [
    "**Feature Selection**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01958408-9568-4fec-8da8-7a2c2c108683",
   "metadata": {},
   "source": [
    "Use the chi-square statistic to find which terms from the vocabulary are mostly related with the sentiment of a movie review (“positive” or “negative”)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8a7b2582-992a-4062-b259-67ef58013d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X, y arrays from training data\n",
    "\n",
    "# Features\n",
    "X_train = df[\"clean_text\"].values\n",
    "\n",
    "# Target values\n",
    "y_train = df[\"label\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e2da2159-28cb-4414-be44-5098f598055d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of words vectors (before feature selection):      (7985, 121507)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of CountVectorizer\n",
    "vectorizer = CountVectorizer(\n",
    "    min_df=2,  # ignore words that appear in less than 2 reviews\n",
    "    ngram_range=(1, 2),  # consider unigrams and bigrams in the vocabulary\n",
    ")\n",
    "\n",
    "# Apply BoW to corpus\n",
    "X_bow = vectorizer.fit_transform(X_train)\n",
    "print(\"Bag of words vectors (before feature selection):      {}\\n\".format(X_bow.shape))\n",
    "\n",
    "# Extract vocabulary\n",
    "vocab = vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160a33ab-3dfb-4782-82cc-060077a9bcf4",
   "metadata": {},
   "source": [
    "Including all unigrams and bigrams in the training data and excluding rare terms resulted in a vocabulary of size 121’519.\n",
    "\n",
    "Next, we select the top 15’000 terms in the vocabulary, according to the chi-square test, i.e., the most related terms with the sentiment of the movie reviews. We will use the feature_selection.SelectKBest and the feature_selection.chi2 methods from scikit-learn to select vocabulary terms with the highest value for the chi-squared statistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6ea15cc3-e6e1-49ce-acb8-7edf0a0d4323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of words vectors after feature selection: (7985, 15000)\n"
     ]
    }
   ],
   "source": [
    "# Feature selection according to chi-squared statistics\n",
    "\n",
    "# Import necessary libraries\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "# Create an instance of the model\n",
    "ch2 = SelectKBest(chi2, k=15000)\n",
    "\n",
    "# Apply fit_transform method to training data\n",
    "X_bow_chi2 = ch2.fit_transform(X_bow, y_train)\n",
    "\n",
    "print(\"Bag of words vectors after feature selection: {}\".format(X_bow_chi2.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04224203-3de2-4bff-b25c-312f07c2aa60",
   "metadata": {},
   "source": [
    "We can see that the chi-square test favors terms expressing sentiment, such as: bad, worst, great, waste, love, waste time, highly recommend, etc. These words are highly connected with the label of movie reviews (“positive” or “negative”) and we expect them to be useful features for predicting the sentiment of a movie review."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c8570d-7cf2-4531-aab4-2f1b34156540",
   "metadata": {},
   "source": [
    "*Tf-idf Term Weighting*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f4b541-19ad-44d8-b9a4-9eac6e949ee3",
   "metadata": {},
   "source": [
    "The main idea behind Tf-idf is that words that are very frequent across all of documents in the corpus aren’t necessarily “informative” to solve a task and they could shadow rarer yet more interesting terms.\n",
    "\n",
    "In short, tf-idf term weighting favors terms that are frequent in few documents, and can be implemented with the TfidfVectorizer object from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f5c46232-c383-4199-b170-bc96e6f74be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weighting text data with Tf-Idf\n",
    "\n",
    "# Import necessary libraries\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create an instance of TfidfVectorizer\n",
    "tfidf = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "29332a6d-7db3-49aa-ac34-3fc3cd5bb61d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape:      (7985, 31906)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# X_tfidf: the tf-idf representation of the data\n",
    "X_tfidf = tfidf.fit_transform(X_train)\n",
    "\n",
    "print(\"Shape:      {}\\n\".format(X_tfidf.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96e641c-2f00-4fe3-b759-68f700c04b41",
   "metadata": {},
   "source": [
    "**Machine Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e578936e-0bf5-437f-9965-ca3a5592e82e",
   "metadata": {},
   "source": [
    "Train a sentiment classifier using a logistic regression model to automatically label movie reviews as positive or negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "19c6f943-fd0a-467c-b395-88ed8ba98115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: (7985,) (7985,)\n",
      "Test data: (7962,) (7962,)\n"
     ]
    }
   ],
   "source": [
    "# Create X, y arrays\n",
    "\n",
    "# Training data\n",
    "X_train = df[\"clean_text\"].values\n",
    "y_train = df[\"label\"].values\n",
    "\n",
    "# Test data\n",
    "X_test = df_test[\"clean_text\"].values\n",
    "y_test = df_test[\"label\"].values\n",
    "\n",
    "print(\"Training data:\", X_train.shape, y_train.shape)\n",
    "print(\"Test data:\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "43668d40-e12a-4153-92aa-b8f236314124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b6f5987f-7d9c-4fc3-b182-7c6a4b5068b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pipeline\n",
    "pipe = Pipeline(\n",
    "    [\n",
    "        (\"vect\", None),\n",
    "        (\"feature_selection\", SelectKBest(score_func=chi2, k=15000)),\n",
    "        (\"clf\", LogisticRegression(solver=\"liblinear\")),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7ef73984-20cb-425b-93b8-7c12d128838a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the grid search cross validator\n",
    "grid_cv = GridSearchCV(\n",
    "    pipe,\n",
    "    param_grid={\n",
    "        \"vect\": [TfidfVectorizer(min_df=2), CountVectorizer(min_df=2)],\n",
    "        \"vect__ngram_range\": [(1, 1), (1, 2)],\n",
    "        \"clf__C\": np.logspace(-4, 4, 6),\n",
    "    },\n",
    "    cv=3,\n",
    "    return_train_score=True,\n",
    "    scoring=\"accuracy\",  # Use the accuracy as metric\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bc725c-0e46-4d1b-9038-8f1d833bef9c",
   "metadata": {},
   "source": [
    "Please note that we are using a small grid to reduce the amount of computation time required to fit the GridSearchCV object below. However, in practice, we would use cv=5 or 10, which are the recommended values when performing k-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "20c51a9e-9319-408e-8feb-e407974c182c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3,\n",
       "             estimator=Pipeline(steps=[('vect', None),\n",
       "                                       ('feature_selection',\n",
       "                                        SelectKBest(k=15000,\n",
       "                                                    score_func=<function chi2 at 0x0000026500FCCE50>)),\n",
       "                                       ('clf',\n",
       "                                        LogisticRegression(solver='liblinear'))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'clf__C': array([1.00000000e-04, 3.98107171e-03, 1.58489319e-01, 6.30957344e+00,\n",
       "       2.51188643e+02, 1.00000000e+04]),\n",
       "                         'vect': [TfidfVectorizer(min_df=2, ngram_range=(1, 2)),\n",
       "                                  CountVectorizer(min_df=2)],\n",
       "                         'vect__ngram_range': [(1, 1), (1, 2)]},\n",
       "             return_train_score=True, scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit estimator\n",
    "grid_cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "40dc5350-6ef0-4661-9d5f-ca228960d421",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vect', TfidfVectorizer(min_df=2, ngram_range=(1, 2))),\n",
       "                ('feature_selection',\n",
       "                 SelectKBest(k=15000,\n",
       "                             score_func=<function chi2 at 0x0000026500FCCE50>)),\n",
       "                ('clf',\n",
       "                 LogisticRegression(C=251.18864315095823, solver='liblinear'))])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show best estimator\n",
    "grid_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "085e8310-a86e-4ec0-bfa5-4966d8174d65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vectorizer</th>\n",
       "      <th>n_gram</th>\n",
       "      <th>C</th>\n",
       "      <th>mean_train_accuracy</th>\n",
       "      <th>std_train_accuracy</th>\n",
       "      <th>mean_validation_accuracy</th>\n",
       "      <th>std_validation_accuracy</th>\n",
       "      <th>mean_fit_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>251.188643</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.908829</td>\n",
       "      <td>0.004977</td>\n",
       "      <td>3.603685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.906699</td>\n",
       "      <td>0.005580</td>\n",
       "      <td>3.859512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>6.309573</td>\n",
       "      <td>0.982092</td>\n",
       "      <td>0.001150</td>\n",
       "      <td>0.903068</td>\n",
       "      <td>0.005607</td>\n",
       "      <td>3.494480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>6.309573</td>\n",
       "      <td>0.994302</td>\n",
       "      <td>0.000354</td>\n",
       "      <td>0.900813</td>\n",
       "      <td>0.010378</td>\n",
       "      <td>1.373275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>251.188643</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.896806</td>\n",
       "      <td>0.008004</td>\n",
       "      <td>1.501868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.893425</td>\n",
       "      <td>0.008103</td>\n",
       "      <td>1.610176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>0.158489</td>\n",
       "      <td>0.989292</td>\n",
       "      <td>0.000934</td>\n",
       "      <td>0.888040</td>\n",
       "      <td>0.009456</td>\n",
       "      <td>4.084579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>6.309573</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.884283</td>\n",
       "      <td>0.007382</td>\n",
       "      <td>4.284116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>0.158489</td>\n",
       "      <td>0.992549</td>\n",
       "      <td>0.000871</td>\n",
       "      <td>0.880024</td>\n",
       "      <td>0.008137</td>\n",
       "      <td>1.621868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>251.188643</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.879774</td>\n",
       "      <td>0.006344</td>\n",
       "      <td>4.465718</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         vectorizer  n_gram             C  mean_train_accuracy  \\\n",
       "17  TfidfVectorizer  (1, 2)    251.188643             1.000000   \n",
       "21  TfidfVectorizer  (1, 2)  10000.000000             1.000000   \n",
       "13  TfidfVectorizer  (1, 2)      6.309573             0.982092   \n",
       "12  TfidfVectorizer  (1, 1)      6.309573             0.994302   \n",
       "16  TfidfVectorizer  (1, 1)    251.188643             1.000000   \n",
       "20  TfidfVectorizer  (1, 1)  10000.000000             1.000000   \n",
       "11  CountVectorizer  (1, 2)      0.158489             0.989292   \n",
       "15  CountVectorizer  (1, 2)      6.309573             1.000000   \n",
       "10  CountVectorizer  (1, 1)      0.158489             0.992549   \n",
       "19  CountVectorizer  (1, 2)    251.188643             1.000000   \n",
       "\n",
       "    std_train_accuracy  mean_validation_accuracy  std_validation_accuracy  \\\n",
       "17            0.000000                  0.908829                 0.004977   \n",
       "21            0.000000                  0.906699                 0.005580   \n",
       "13            0.001150                  0.903068                 0.005607   \n",
       "12            0.000354                  0.900813                 0.010378   \n",
       "16            0.000000                  0.896806                 0.008004   \n",
       "20            0.000000                  0.893425                 0.008103   \n",
       "11            0.000934                  0.888040                 0.009456   \n",
       "15            0.000000                  0.884283                 0.007382   \n",
       "10            0.000871                  0.880024                 0.008137   \n",
       "19            0.000000                  0.879774                 0.006344   \n",
       "\n",
       "    mean_fit_time  \n",
       "17       3.603685  \n",
       "21       3.859512  \n",
       "13       3.494480  \n",
       "12       1.373275  \n",
       "16       1.501868  \n",
       "20       1.610176  \n",
       "11       4.084579  \n",
       "15       4.284116  \n",
       "10       1.621868  \n",
       "19       4.465718  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Collect most interesting results in a DataFrame\n",
    "cv_results = pd.DataFrame(\n",
    "    {\n",
    "        \"vectorizer\": [type(clf).__name__ for clf in grid_cv.cv_results_[\"param_vect\"]],\n",
    "        \"n_gram\": [k for k in grid_cv.cv_results_[\"param_vect__ngram_range\"]],\n",
    "        \"C\": [C for C in grid_cv.cv_results_[\"param_clf__C\"]],\n",
    "        \"mean_train_accuracy\": grid_cv.cv_results_[\"mean_train_score\"],\n",
    "        \"std_train_accuracy\": grid_cv.cv_results_[\"std_train_score\"],\n",
    "        \"mean_validation_accuracy\": grid_cv.cv_results_[\"mean_test_score\"],\n",
    "        \"std_validation_accuracy\": grid_cv.cv_results_[\"std_test_score\"],\n",
    "        \"mean_fit_time\": grid_cv.cv_results_[\"mean_fit_time\"],\n",
    "    }\n",
    ")\n",
    "\n",
    "# Print grid-search results\n",
    "cv_results.sort_values(\"mean_validation_accuracy\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6c8389-be7c-4efa-9ad5-f09117f43548",
   "metadata": {},
   "source": [
    "We can see that the Tf-idf model performs better than the Bag of words model with the dataset. Moreover, including unigrams and bi-grams into our vocabulary was beneficial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "95fb3a6a-6172-44ec-b219-35329afd161f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test data: 85.10%\n"
     ]
    }
   ],
   "source": [
    "# Compute accuracy on test data\n",
    "print(\"Accuracy on test data: {0:.2f}%\".format(100 * grid_cv.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "bb0cf537-f088-4511-b9be-d3480c598667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.89      0.86      3987\n",
      "           1       0.88      0.82      0.85      3975\n",
      "\n",
      "    accuracy                           0.85      7962\n",
      "   macro avg       0.85      0.85      0.85      7962\n",
      "weighted avg       0.85      0.85      0.85      7962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Classification report\n",
    "print(classification_report(y_test, grid_cv.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef7ea92-3537-4711-a2c2-9e46ab20bc99",
   "metadata": {},
   "source": [
    "Our model has similar f1-scores for both classes. The differences between the two classes are minor; our model classifies correctly slightly less positive than negative reviews. However, our model makes slightly fewer mistakes when predicting positive reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c2b19c2e-17da-4eea-8ecd-8f1a22d4b287",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "# Create two subplots\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Plots the standard confusion matrix\n",
    "ax1.set_title(\"Confusion Matrix (counts)\")\n",
    "ConfusionMatrixDisplay.from_predictions(\n",
    "    y_test, grid_cv.predict(X_test), display_labels=[0,1], ax=ax1)\n",
    "\n",
    "# Plots the normalized confusion matrix\n",
    "ax2.set_title(\"Confusion Matrix (ratios)\")\n",
    "ConfusionMatrixDisplay.from_predictions(\n",
    "    y_test, grid_cv.predict(X_test), normalize=\"true\", display_labels=[0,1], ax=ax2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d884f1-1832-4f82-994b-e54194d73d14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:adsml] *",
   "language": "python",
   "name": "conda-env-adsml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
